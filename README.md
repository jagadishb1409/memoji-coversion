# Memoji-coversion

## Introduction

<p class="text-justify"> Facial expression for emotion detection has always been an easy task for humans, but achieving
the same task with a computer algorithm is quite challenging. With the recent advancement
in computer vision and machine learning, it is possible to detect emotions from images. Computer
algorithms are very sophisticated. Intelligent human-computer interaction is a new field
aimed at enabling people to use computers naturally as tools. It is claimed that human communication
skills are required for computers to interact with humans. One of these skills is the
ability to understand a person’s emotional state. Therefore a utility that detects emotion from
facial expressions would be widely applicable. Such an advancement could bring applications
in medicine, marketing and entertainment. </p>

## Objectives

i. To develop a CNN model for detecting facial expressions to classify them into seven emotions
like Angry, Disgust, Fear, Happy, Sad, Surprise and Neutral.

ii. To develop a website that can convert facial expressions to personalised emoji’s
at real time using Facecam.

## Problem statement

To develop a website to convert facial expression to memoji in real time.

## Application in Societal Context

i. Website can be used to create personalized memoji.

ii. Can be used to generate animated face for videos.

iii. Memoji is used to create NFT images as the profile pictures.


## Block diagram

<p align="center">
  <img src="https://user-images.githubusercontent.com/63513035/176335965-d2e9bfa8-f8a0-4a3d-9e0f-c439ca5a2f62.png">
</p>

After the input is fed in the form of image, the background is removed so that it cannot
compromise or reduce the prediction accuracy. So, the proposed system works on two level 
of CNN framework. The first level removes the background and the second level extracts the
primary expression vector (EV). The EV is generated by tracking down the relevant facial
points of importance. Each layer consists of 4 filters. These filters detect the shapes, edges,
texture and objects. The first layer uses edge, circle , corner detector filter to detect the face
while the second layer catches the eyes, lips, ears, nose etc. Above mentioned technique is
based on FACS(Facial Action Coding System) which leverages the relevant facial points to
detect the human emotion instead of wearing sophisticated sensors.












